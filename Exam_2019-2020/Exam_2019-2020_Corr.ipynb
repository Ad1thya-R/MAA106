{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<img src='./figures/logo-ecole-polytechnique-ve.jpg' style='position:absolute; top:0; right:0;' width='100px' height='' alt='' />\n",
    "\n",
    "<center><b>Bachelor of Ecole Polytechnique</b></center>\n",
    "<center>Computational Mathematics, year 1, semester 2</center>\n",
    "<center>Author: Aline Lefebvre-Lepot</center>\n",
    "\n",
    "# EXAM (MAA106 2019-2020)\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<center><b>Due date: XXX</b></center>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div markdown=1 class=Abstract> \n",
    "\n",
    "**This subject is about acceleration of convergence and consists of 3 sections:**\n",
    "\n",
    "- Section 1: Initial algorithm\n",
    "- Section 2: Relaxed algorithm\n",
    "- Section 3: Approximated relaxed algorithm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div markdown=1 class=Abstract> \n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "- Justify (even with a few words) your answers\n",
    "- Please provide your answers in the \"Answer\" cells already created\n",
    "- Upload your notebook to the designated area on moodle after you have finished\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Reminder: If the notebook does not have the usual aspect, for instance if you do not have cells of different colors, click on the \"not trusted\" button in the top right corner.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## loading python libraries\n",
    "\n",
    "# necessary to display plots inline:\n",
    "%matplotlib inline   \n",
    "\n",
    "# load the libraries\n",
    "import matplotlib.pyplot as plt # 2D plotting library\n",
    "import numpy as np              # package for scientific computing  \n",
    "\n",
    "from math import *              # package for mathematics (pi, arctan, sqrt, factorial ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "A function $g$ being given, we consider the problem of finding $x^*$ solution to \n",
    "\n",
    "$$\n",
    "g(x^*)=x^*.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## A first algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We provide below the code of an algorithm. Given a function $g$, a real $x_0$, a real $\\epsilon$ and an integer $K$, it computes a sequence $(x_k)_k$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## input : g = function\n",
    "##         x0 = real\n",
    "##         eps = real\n",
    "##         K = integer\n",
    "## output : kend = the number of iteration achieved\n",
    "##          x = array [x0, ... , xkend]\n",
    "def Algo(g,x0,eps,K):\n",
    "    x = np.zeros(K+1)\n",
    "    x[0] = x0 \n",
    "    x[1] = g(x0)\n",
    "    k = 1\n",
    "    while abs(x[k]-x[k-1])>eps and k < K:\n",
    "        x[k+1] = g(x[k])\n",
    "        k = k+1\n",
    "    return (k, x[:k+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div  markdown=1 class=\"DoIt\"> Answer the following questions:\n",
    "\n",
    "- How is $x_{k+1}$ computed from $x_k$?\n",
    "- When does the algorithm stop?\n",
    "- If the sequence $x_k$ converges to a real $x^*$, and the function $g$ is continuous, which equation is verified by the limit $x^*$?\n",
    "- How is this algorithm called?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div  markdown=1 class=\"Answer\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div  markdown=1 class=\"Correction\"> \n",
    "\n",
    "- The relationship defined by the algorithm writes: $x_{k+1} = g(x_k)$ \n",
    "- The algorithm stops when either $|x_k-x_{k-1}|<\\epsilon$, or $k\\geq K$.\n",
    "- If the function $g$ is continuous and the limit  $x^*$ exists, it satisfies $g(x^*)=g(\\lim_{k\\rightarrow +\\infty} x_k)=\\lim_{k\\rightarrow +\\infty} g(x_k) = \\lim_{k\\rightarrow +\\infty} x_{k+1} = x^*$. Hence $g(x^*)=x^*$.\n",
    "- This algorithm is called the fixed point algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We recall the following result about this algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div  markdown=1 class=\"Thm\">\n",
    "\n",
    "**Convergence of the algorithm.**\n",
    "\n",
    "Let $g: [a,b]\\to \\mathbb{R}$ and $x_0$ be given. Consider the sequence $(x_{k})_k$ for $k\\geq 0$, generated by the algorithm. Suppose\n",
    "\n",
    "- $g: [a,b] \\to [a,b]\\quad$ (i.e. $[a,b]$ is stable for $g$)\n",
    "- $g\\in {\\cal C}^1 ([a,b])$ and\n",
    "$$\n",
    "\\exists K<1 \\quad\\ \\text{such that} \\quad\\ \\forall x\\in[a,b], \\quad|g'(x)|\\leq K\n",
    "$$\n",
    "\n",
    "Then,  the sequence $(x_k)_k$ converges to the same limit $x^*$ for any choice of $x_0\\in [a,b]$. Moreover we have\n",
    "\n",
    "$$\n",
    "\\lim_{k\\to\\infty} \\frac{x_{k+1}-x^*}{x_k-x^*} = g'(x^*)\n",
    "$$\n",
    "\n",
    "so that the sequence converges at least with order 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div  markdown=1 class=\"DoIt\">\n",
    "\n",
    "We consider the function $g:[1,2]\\to\\mathbb{R}$, defined by:\n",
    "\n",
    "$$\\displaystyle g(x) = -\\frac{1}{20} (x^3-2) + x \\qquad\\forall~x\\in[1,2].$$ \n",
    "\n",
    "- Compute $g'$\n",
    "- Prove that $g$ is an increasing function\n",
    "- Prove that $g([1,2])\\subset [1,2]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div  markdown=1 class=\"Answer\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div  markdown=1 class=\"Correction\">\n",
    "\n",
    "- $g'(x) = -3x^2/20 + 1$.\n",
    "- The derivative $g'$ is a quadratic polynomial, having roots at $x_\\pm=\\pm\\sqrt{\\frac{20}{3}}$. Since the coefficient of order 2, namely $-3/20$ is negative, $g'$ is positive between $x_-$ and $x_+$. $[1,2]$ being included in $[x_-,x_+]$, $g'$ is positive on $[1,2]$, and therefore $g$ is increasing on $[1,2]$.\n",
    "- Since $g$ in increasing, we have $g([1,2])=[g(1),g(2)]=\\left[\\frac{21}{20},\\frac{17}{10}\\right]\\subset[1,2]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div  markdown=1 class=\"DoIt\">\n",
    "\n",
    "We want to apply the algorithm to the function $g$. To do so we choose $x_0\\in [1,2]$.\n",
    "\n",
    "- Using the previous theorem, prove that the sequence generated by the algorithm converges to a real $x^*$ with order at least 1.\n",
    "- What is the value of $x^*$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div  markdown=1 class=\"Answer\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div  markdown=1 class=\"Correction\"> \n",
    "\n",
    "- $g$ satisfies the theorem's hypotheses: $g$ is $\\mathcal C^1$, $[1,2]$ is stable for $g$, and we can bound the derivative of $g$: since $x^2$ is increasing between $1$ and $2$, $g'$ is decreasing. We have $g'(1) \\geq g'(x) \\geq g'(2)$, that is $17/20 \\geq g'(x) \\geq 2/5$. Hence $|g'(x)| < 17/20 < 1$ between $1$ and $2$. The theorem thus applies for $g$, and the sequence converges with order at least one.\n",
    "- From the equation $g(x^*)=x^*$, we deduce: $(x^*)^3-2=0 \\iff x^*=2^{1/3}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div  markdown=1 class=\"DoIt\">\n",
    "\n",
    "Run the following cell to compute the first terms of the sequence for the function $g$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xstar = 1.2599210498948732\n",
      "k =  24\n",
      "x = 1.260198298526237\n",
      "Last test = 8.668017933266547e-05\n",
      "Last error = 0.0002772486313638023\n"
     ]
    }
   ],
   "source": [
    "# solution\n",
    "xstar = 2**(1/3)\n",
    "print('xstar =',xstar)\n",
    "\n",
    "# definition of function g\n",
    "def g(x):\n",
    "    return - (x**3-2)/20 + x\n",
    "\n",
    "# parameters\n",
    "x0 = 1.5\n",
    "eps = 1e-4\n",
    "K = 30\n",
    "\n",
    "# run the algorithm\n",
    "k, x = Algo(g,x0,eps,K)\n",
    "\n",
    "# print number of terms computed\n",
    "print('k = ',k)\n",
    "# print the last approximation computed\n",
    "print('x =',x[k])\n",
    "# print the last test performed in the loop\n",
    "FinalTest = abs(x[k]-x[k-1])\n",
    "print('Last test =',FinalTest)\n",
    "# print the error for the last approximation\n",
    "err = abs(x[k]-xstar)\n",
    "print('Last error =',err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div  markdown=1 class=\"DoIt\">\n",
    "\n",
    "In the previous test: \n",
    "- Compare the number of terms computed with $K$.\n",
    "- Compare the value of $\\epsilon$ with the last test in the loop.\n",
    "- Why did the algorithm stop? \n",
    "- Compare the value of the error with the last test.\n",
    "- Compare the value of the error with $\\epsilon$.\n",
    "\n",
    "In view of this test, \n",
    "- Does the stopping criterion $|x_k-x_{k-1}|<\\epsilon$ ensure that the precision of the last approximation is smaller than $\\epsilon$?\n",
    "- Is $|x_k-x_{k-1}|$ an \"error estimator\" for this algorithm? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div  markdown=1 class=\"Answer\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div  markdown=1 class=\"Correction\"> \n",
    "\n",
    "- The algorithm stopped before $K$ iterations: $k_{end}<K$\n",
    "- The value for the last test is smaller than $\\epsilon$: $|x_{k_{end}}-x_{{k_{end}}-1}|<\\epsilon$\n",
    "- The algorithm stopped because the desired precision $|x_{k_{end}}-x_{{k_{end}}-1}|<\\epsilon$ was reached before the $K$-th iteration.\n",
    "- We can see that $err= |x_{k_{end}}-x^*| >|x_{k_{end}}-x_{{k_{end}}-1}|$\n",
    "- We also have $err>\\epsilon$\n",
    "- The algorithm brings no guarantee about the precision of the last approximation: here, it stopped because of the stopping criterion $|x_{k_{end}}-x_{{k_{end}}-1}|<\\epsilon$, but still had an error $|x^*-x_k|$ larger than $\\epsilon$.\n",
    "- An error estimator is an upper bound for the error. Here, the error can be greater than $|x_k-x_{k-1}|$: it is not an error estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div  markdown=1 class=\"DoIt\">\n",
    "\n",
    "Complete the following cell to plot the error versus $k$ for the previous test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (234442125.py, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;36m  Input \u001B[0;32mIn [12]\u001B[0;36m\u001B[0m\n\u001B[0;31m    err = ---\u001B[0m\n\u001B[0m             ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# solution\n",
    "xstar = 2**(1/3)\n",
    "\n",
    "# parameters\n",
    "x0 = 1.5\n",
    "eps = 1e-4\n",
    "K = 30\n",
    "\n",
    "# run the algorithm\n",
    "k, x = Algo(g,x0,eps,K)\n",
    "\n",
    "## vector e_{k} = |x_k-x^*|\n",
    "err = ---\n",
    "\n",
    "## plot e_{k} versus k\n",
    "tabk = np.arange(0,k+1)\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "plt.plot(---,---, 'o-', label='First Algorithm') #plot\n",
    "plt.legend(loc='lower right', fontsize=18)\n",
    "plt.xlabel(---, fontsize=18)\n",
    "plt.ylabel(---, fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div  markdown=1 class=\"Correction\"> Code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# solution\n",
    "xstar = 2**(1/3)\n",
    "\n",
    "# parameters\n",
    "x0 = 1.5\n",
    "eps = 1e-4\n",
    "K = 30\n",
    "\n",
    "# run the algorithm\n",
    "k, x = Algo(g,x0,eps,K)\n",
    "\n",
    "## vector e_{k} = |x_k-x^*|\n",
    "err =  abs(x-xstar)\n",
    "\n",
    "## plot e_{k+1} versus k\n",
    "tabk = np.arange(0,k+1)\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "plt.plot(tabk,err, 'o-', label='First Algorithm') #plot\n",
    "plt.legend(loc='upper right', fontsize=18)\n",
    "plt.xlabel('k', fontsize=18)\n",
    "plt.ylabel('err', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div  markdown=1 class=\"DoIt\">\n",
    "\n",
    "Complete the following cell to estimate the order of convergence of the algorithm. Use a figure together with the command polyfit to estimate the order of convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# solution\n",
    "xstar = 2**(1/3)\n",
    "x0 = 1.5\n",
    "eps = 1e-4\n",
    "K = 30\n",
    "k, x = Algo(g,x0,eps,K)\n",
    "\n",
    "## vector e_{k} = |x_k-x^*|\n",
    "err = ---\n",
    "\n",
    "## vectors e_{k} and  e_{k+1}\n",
    "X = ---     # X[k]=e_{k},   k=0...kend-1\n",
    "Y = ---     # Y[k]=e_{k+1}, k=0...kend-1\n",
    "\n",
    "## plot e_{k+1} versus e_{k} in loglog-scale\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "--- #plot, log-log scale\n",
    "plt.legend(loc='lower right', fontsize=18)\n",
    "plt.axis('equal')\n",
    "plt.xlabel(---, fontsize=18)\n",
    "plt.ylabel(---, fontsize=18)\n",
    "plt.show()\n",
    "\n",
    "## estimate the slope of the line using polyfit\n",
    "---\n",
    "print(\"approximated order of convergence = \",---)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div  markdown=1 class=\"Correction\"> Code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# solution\n",
    "xstar = 2**(1/3)\n",
    "x0 = 1.5\n",
    "eps = 1e-4\n",
    "K = 30\n",
    "k, x = Algo(g,x0,eps,K)\n",
    "\n",
    "## vector e_{k} = |x_k-x^*|\n",
    "err = abs(x-xstar)\n",
    "\n",
    "## vectors e_{k} and  e_{k+1}\n",
    "X = err[:-1]    # X[k]=e_{k},   k=0...kend-1\n",
    "Y = err[1:]     # Y[k]=e_{k+1}, k=0...kend-1\n",
    "\n",
    "## plot e_{k+1} versus e_{k} in loglog-scale\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "plt.loglog(X, Y, marker=\"o\", label='First Algorithm') #log-log scale\n",
    "plt.legend(loc='lower right', fontsize=18)\n",
    "plt.axis('equal')\n",
    "plt.xlabel('$e_k$', fontsize=18)\n",
    "plt.ylabel('$e_{k+1}$', fontsize=18)\n",
    "plt.show()\n",
    "\n",
    "## estimates the slope of the line using polyfit\n",
    "fit = np.polyfit(np.log(X), np.log(Y),1)\n",
    "a = fit[0]\n",
    "b = fit[1]\n",
    "print(\"approximated order of convergence = \",a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div  markdown=1 class=\"DoIt\">\n",
    "\n",
    "Does this test confirm the expected order of convergence?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div  markdown=1 class=\"Answer\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div  markdown=1 class=\"Correction\">\n",
    "\n",
    "Yes, we observe convergence with order $1$ (slope = $0.99$) which was expected by the theoretical analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Relaxation method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We recall the following result, showing that, for \"good\" functions $g$, the previous algorithm converges with a better than linear speed of convergence:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div  markdown=1 class=\"Thm\">\n",
    "\n",
    "**\"Better than linear\" speed of convergence.**\n",
    "\n",
    "Let $g: [a,b]\\to \\mathbb{R}$ and suppose that the hypotheses of the previous theorem are fulfilled. If \n",
    "\n",
    "- $g\\in {\\cal C^{p+1}} ( I )$ where $I$ is a neighbourhood of $x^*$ and $p$ is an integer $p\\geq 0$\n",
    "- $g^{(i)}(x^*)=0\\quad$ for $\\quad 0\\leq i \\leq p$\n",
    "- $g^{(p+1)}(x^*)\\neq 0$ \n",
    "\n",
    "Then, the algorithm applied to the function $g$ satisfies \n",
    "\n",
    "$$\n",
    "\\lim_{k\\to\\infty} \\frac{x_{k+1}-x^*}{(\\,x_k-x^*\\,)\\,^{p+1}} = \\frac{g^{(p+1)}(x^*)}{(p+1)!}.\n",
    "$$\n",
    "\n",
    "This proves that the sequence converges at least with order $p+1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The relaxation method tries to take advantage of this result, by modifying the function in order to make the algorithm converge quicker.\n",
    "\n",
    "For any $\\alpha\\neq -1$, consider the following function:\n",
    "\n",
    "$$g_\\alpha(x) = \\frac{g(x)+\\alpha x}{\\alpha +1}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div  markdown=1 class=\"DoIt\">\n",
    "\n",
    "- Prove that $g(x^*)=x^*$ is equivalent to $g_\\alpha(x^*)=x^*$\n",
    "- Compute the derivative of $g_\\alpha$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div  markdown=1 class=\"Answer\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div  markdown=1 class=\"Correction\">\n",
    "\n",
    "- We have $g(x)=x \\Longleftrightarrow \\alpha x + x = \\alpha x + g(x) \\Longleftrightarrow g_\\alpha(x)=x$\n",
    "- The derivative of $g_\\alpha$ is: $g_\\alpha'(x) = \\frac{g'(x)+\\alpha}{\\alpha+1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We choose to approximate $x^*$ solution to $g(x^*)=x^*$ by applying the algorithm studied in the previous section to \n",
    "\n",
    "$$g_\\alpha\\quad \\text{ for } \\quad \\alpha = -g'(x^*).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div  markdown=1 class=\"DoIt\"> Explain this choice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div  markdown=1 class=\"Answer\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div  markdown=1 class=\"Correction\">\n",
    "\n",
    "- Using the algorithm with $g_\\alpha$ computes the same fixed point as for $g$.\n",
    "- From the definition of $\\alpha$, we have $g'_\\alpha(x^*)=0$: in view of the previous theorem, the algorithm will converge with order at least 2 (provided $g$ is at least $\\mathcal{C}^2$ close to $x^*$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This method is called a relaxation method. The corresponding iterations for a given function $g$ are:\n",
    "\n",
    "$$\n",
    "x_{k+1} = g_\\alpha (x_k) = \\frac{g(x_k)+\\alpha x_k}{\\alpha +1}\n",
    "$$\n",
    "\n",
    "The real $\\alpha$ is called the relaxation parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div  markdown=1 class=\"DoIt\"> Complete the function below to implement this new algorithm. Use the same stopping criterion as in the previous section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## input : g = function\n",
    "##         alpha = relaxation parameter\n",
    "##         x0 = real\n",
    "##         eps = real\n",
    "##         K = integer\n",
    "## output : kend = the number of iteration achieved\n",
    "##          x = array [x0, ... , xkend]\n",
    "def RelaxedAlgo(g,alpha,x0,eps,K):\n",
    "    ---\n",
    "    return (k, x[:k+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div  markdown=1 class=\"Correction\"> Code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## input : g = function\n",
    "##         alpha = relaxation parameter\n",
    "##         x0 = real\n",
    "##         eps = real\n",
    "##         K = integer\n",
    "## output : kend = the number of iteration achieved\n",
    "##          x = array [x0, ... , xkend]\n",
    "def RelaxedAlgo(g,alpha,x0,eps,K):\n",
    "    x = np.zeros(K+1)\n",
    "    x[0] = x0 \n",
    "    x[1] = (g(x0) + alpha * x0) / (alpha +1)\n",
    "    k = 1\n",
    "    while abs(x[k]-x[k-1])>eps and k < K:\n",
    "        x[k+1] = (g(x[k]) + alpha * x[k]) / (alpha +1)\n",
    "        k = k+1\n",
    "    return (k, x[:k+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div  markdown=1 class=\"DoIt\"> \n",
    "Test this relaxed algorithm for\n",
    "$$\\displaystyle g(x) = -\\frac{1}{20} (x^3-2) + x.$$\n",
    "\n",
    "Using the same choice of parameters as before, plot on the same figure the error versus $k$ for both the initial and the new algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# solution\n",
    "xstar = 2**(1/3)\n",
    "\n",
    "# parameters\n",
    "x0 = 1.5\n",
    "eps = 1e-4\n",
    "K = 30\n",
    "\n",
    "# run the initial algorithm\n",
    "k, x = ---\n",
    "\n",
    "# run the relaxed algorithm\n",
    "alpha = ---\n",
    "k_rel, x_rel = ---\n",
    "\n",
    "## errors\n",
    "err =  ---\n",
    "err_rel =  ---\n",
    "\n",
    "## plot e_{k+1} versus k\n",
    "tabk = np.arange(0,k+1)\n",
    "tabk_rel = np.arange(0,k_rel+1)\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "--- #plot first algorithm\n",
    "--- #plot relaxed algorithm\n",
    "plt.legend(loc='upper right', fontsize=18)\n",
    "plt.xlabel(---, fontsize=18)\n",
    "plt.ylabel(---, fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div  markdown=1 class=\"Correction\"> Code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# solution\n",
    "xstar = 2**(1/3)\n",
    "\n",
    "# parameters\n",
    "x0 = 1.5\n",
    "eps = 1e-4\n",
    "K = 30\n",
    "\n",
    "# run the initial algorithm\n",
    "k, x = Algo(g,x0,eps,K)\n",
    "\n",
    "# run the relaxed algorithm\n",
    "alpha = (3*xstar**2)/20 - 1\n",
    "k_rel, x_rel = RelaxedAlgo(g,alpha,x0,eps,K)\n",
    "\n",
    "## errors\n",
    "err =  abs(x-xstar)\n",
    "err_rel =  abs(x_rel-xstar)\n",
    "\n",
    "## plot e_{k+1} versus k\n",
    "tabk = np.arange(0,k+1)\n",
    "tabk_rel = np.arange(0,k_rel+1)\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "plt.plot(tabk,err, 'o-', label='First Algorithm') #plot\n",
    "plt.plot(tabk_rel,err_rel, 'o-', label='Relaxed Algorithm') #plot\n",
    "plt.legend(loc='upper right', fontsize=18)\n",
    "plt.xlabel('k', fontsize=18)\n",
    "plt.ylabel('err', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div  markdown=1 class=\"DoIt\"> \n",
    "Compare the convergence of the two algorithms: speed of convergence, number of iterations obtained for the same value of epsilon, error for the last computed approximation. (print some of the computed results if needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div  markdown=1 class=\"Answer\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div  markdown=1 class=\"Correction\"> As expected, the relaxed algorithm converges much faster.\n",
    "\n",
    "- With the initial algorithm: $k_{end} = 24$, error about $2.7e-4$ (see below)\n",
    "- With the relaxed algorithm: $k_{end} = 5$, error about $2.4e-14$ (see below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(k,err[k])\n",
    "print(k_rel,err_rel[k_rel])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div  markdown=1 class=\"DoIt\"> Complete the next cell to estimate the order of convergence of the relaxed algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "xstar = 2**(1/3)\n",
    "x0 = 1.5\n",
    "eps = 1e-4\n",
    "K = 30\n",
    "\n",
    "---\n",
    "\n",
    "print(\"approximated order of convergence = \",---)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div  markdown=1 class=\"Correction\"> Code below (the plot is not mandatory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# solution\n",
    "xstar = 2**(1/3)\n",
    "x0 = 1.5\n",
    "eps = 1e-4\n",
    "K = 30\n",
    "alpha = (3*xstar**2)/20 - 1\n",
    "k, x = RelaxedAlgo(g,alpha,x0,eps,K)\n",
    "\n",
    "## vector e_{k} = |x_k-x^*|\n",
    "err = abs(x-xstar)\n",
    "\n",
    "## vectors e_{k} and  e_{k+1}\n",
    "X = err[:-1]    # X[k]=e_{k},   k=0...kend-1\n",
    "Y = err[1:]     # Y[k]=e_{k+1}, k=0...kend-1\n",
    "\n",
    "## plot e_{k+1} versus e_{k} in loglog-scale\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "plt.loglog(X, Y, marker=\"o\", label='Relaxed Algorithm') #log-log scale\n",
    "plt.legend(loc='lower right', fontsize=18)\n",
    "plt.axis('equal')\n",
    "plt.xlabel('$e_k$', fontsize=18)\n",
    "plt.ylabel('$e_{k+1}$', fontsize=18)\n",
    "plt.show()\n",
    "\n",
    "## estimates the slope of the line using polyfit\n",
    "fit = np.polyfit(np.log(X), np.log(Y),1)\n",
    "a = fit[0]\n",
    "b = fit[1]\n",
    "print(\"approximated order of convergence = \",a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div  markdown=1 class=\"DoIt\">\n",
    "\n",
    "Does this test confirm the expected order of convergence?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div  markdown=1 class=\"Answer\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div  markdown=1 class=\"Correction\">\n",
    "\n",
    "Yes, we observe convergence with order 2 which was expected by the theoretical analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Approximated relaxation method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Sometimes, the derivative of the function is not explicit or its derivative is not known.\n",
    "\n",
    "In that case, the relaxation method can be modified by estimating the derivative using a finite difference approximation:\n",
    "\n",
    "Find $a,b$ such that $a\\approx x^*$, $b\\approx x^*$ and chose\n",
    "$$\n",
    "\\alpha = - \\frac{g(b)-g(a)}{b-a}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div  markdown=1 class=\"DoIt\"> How could $a$ and $b$ be chosen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div  markdown=1 class=\"Answer\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div  markdown=1 class=\"Correction\"> $a$ and $b$ can be chosen by running a few steps of a (less efficient) method, like the initial fixed point algorithm, which does not requires to know the derivative. This would provide a first guess $\\hat x$ close to $x^*$ and one can choose $a=\\hat x - \\eta$ and $b = \\hat x + \\eta$ for $\\eta$ small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div  markdown=1 class=\"DoIt\"> Test this method in the following cell. \n",
    "\n",
    "- For the sake of simplicity, and since for this academic example $x^*$ is known, choose $a=x^* - \\eta$ and $b = x^* + \\eta$ for $\\eta= 5.10^{-1}$.\n",
    "\n",
    "- Plot on the same figure the error versus $k$ for the initial algorithm, the relaxed method and the approximated relaxed method. Use a log-scale for the error (y-axis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# solution\n",
    "xstar = 2**(1/3)\n",
    "\n",
    "# parameters\n",
    "x0 = 1.5\n",
    "eps = 1e-4\n",
    "K = 30\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div  markdown=1 class=\"Correction\"> Code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# solution\n",
    "xstar = 2**(1/3)\n",
    "\n",
    "# parameters\n",
    "x0 = 1.5\n",
    "eps = 1e-4\n",
    "K = 30\n",
    "\n",
    "# run the initial algorithm\n",
    "k, x = Algo(g,x0,eps,K)\n",
    "\n",
    "# run the relaxed algorithm\n",
    "alpha = (3*xstar**2)/20 - 1\n",
    "k_rel, x_rel = RelaxedAlgo(g,alpha,x0,eps,K)\n",
    "\n",
    "# run the approximated relaxed algorithm\n",
    "eta = 5e-1\n",
    "alpha2 = -(g(xstar+eta)-g(xstar-eta))/(2*eta)\n",
    "k_rel2, x_rel2 = RelaxedAlgo(g,alpha2,x0,eps,K)\n",
    "\n",
    "## errors\n",
    "err =  abs(x-xstar)\n",
    "err_rel =  abs(x_rel-xstar)\n",
    "err_rel2 =  abs(x_rel2-xstar)\n",
    "\n",
    "## plot e_{k+1} versus k\n",
    "tabk = np.arange(0,k+1)\n",
    "tabk_rel = np.arange(0,k_rel+1)\n",
    "tabk_rel2 = np.arange(0,k_rel2+1)\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "plt.plot(tabk,err, 'o-', label='First Algorithm') #plot\n",
    "plt.plot(tabk_rel,err_rel, 'o-', label='Relaxed Algorithm') #plot\n",
    "plt.plot(tabk_rel2,err_rel2, 'o-', label='Approximated Relaxed Algorithm') #plot\n",
    "plt.legend(loc='upper right', fontsize=18)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('k', fontsize=18)\n",
    "plt.ylabel('err', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div  markdown=1 class=\"DoIt\"> Comment on the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div  markdown=1 class=\"Answer\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div  markdown=1 class=\"Correction\"> The relaxed fixed point algorithm is better than the approximated relaxed algorithm, which is itself better than initial algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div  markdown=1 class=\"DoIt\"> Complete the next cell to estimate the order of convergence of the approximated relaxed algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "xstar = 2**(1/3)\n",
    "x0 = 1.5\n",
    "eps = 1e-8\n",
    "K = 30\n",
    "eta = 5e-1\n",
    "\n",
    "---\n",
    "\n",
    "print(\"approximated order of convergence = \",---)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div  markdown=1 class=\"Correction\"> Code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# solution\n",
    "xstar = 2**(1/3)\n",
    "x0 = 1.5\n",
    "eps = 1e-8\n",
    "K = 30\n",
    "eta = 5e-1\n",
    "alpha = -(g(xstar+eta)-g(xstar-eta))/(2*eta)\n",
    "k, x = RelaxedAlgo(g,alpha,x0,eps,K)\n",
    "\n",
    "## vector e_{k} = |x_k-x^*|\n",
    "err = abs(x-xstar)\n",
    "\n",
    "## vectors e_{k} and  e_{k+1}\n",
    "X = err[:-1]    # X[k]=e_{k},   k=0...kend-1\n",
    "Y = err[1:]     # Y[k]=e_{k+1}, k=0...kend-1\n",
    "\n",
    "## plot e_{k+1} versus e_{k} in loglog-scale\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "plt.loglog(X, Y, marker=\"o\", label='Relaxed Algorithm') #log-log scale\n",
    "plt.legend(loc='lower right', fontsize=18)\n",
    "plt.axis('equal')\n",
    "plt.xlabel('$e_k$', fontsize=18)\n",
    "plt.ylabel('$e_{k+1}$', fontsize=18)\n",
    "plt.show()\n",
    "\n",
    "## estimates the slope of the line using polyfit\n",
    "fit = np.polyfit(np.log(X), np.log(Y),1)\n",
    "a = fit[0]\n",
    "b = fit[1]\n",
    "print(\"approximated order of convergence = \",a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div  markdown=1 class=\"DoIt\">\n",
    "\n",
    "Does this test confirm the expected order of convergence?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div  markdown=1 class=\"Answer\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div  markdown=1 class=\"Correction\">\n",
    "\n",
    "Yes, since $\\alpha \\neq -g'(x^*)$, we have $g'_\\alpha(x^*)\\neq 0$ and we observe convergence with order 1 which was expected by the theoretical analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div  markdown=1 class=\"DoIt\">\n",
    "\n",
    "Complete the following cell to plot on the same graph $e_{k+1}$ versus $e_k$ in a log-log scale for the three methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# solution\n",
    "xstar = 2**(1/3)\n",
    "x0 = 1.5\n",
    "eps = 1e-8\n",
    "K = 30\n",
    "eta = 5e-1\n",
    "\n",
    "#compute iterations, initial algorithm\n",
    "k1, x1 = ---\n",
    "\n",
    "#compute iterations, relaxed algorithm\n",
    "alpha = ---\n",
    "k2, x2 = ---\n",
    "\n",
    "#compute iterations, approximated relaxed algorithm\n",
    "alpha = ---\n",
    "k3, x3 = ---\n",
    "\n",
    "## vector e_{k} = |x_k-x^*|\n",
    "---\n",
    "\n",
    "## vectors e_{k} and  e_{k+1}\n",
    "---\n",
    "\n",
    "## plot e_{k+1} versus e_{k} in loglog-scale\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "---\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div  markdown=1 class=\"Correction\"> Code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# solution\n",
    "xstar = 2**(1/3)\n",
    "x0 = 1.5\n",
    "eps = 1e-8\n",
    "K = 30\n",
    "eta = 5e-1\n",
    "\n",
    "#compute iterations, initial algorithm\n",
    "k1, x1 = Algo(g,x0,eps,K)\n",
    "\n",
    "#compute iterations, relaxed algorithm\n",
    "alpha = (3*xstar**2)/20 - 1\n",
    "k2, x2 = RelaxedAlgo(g,alpha,x0,eps,K)\n",
    "\n",
    "#compute iterations, approximated relaxed algorithm\n",
    "alpha = -(g(xstar+eta)-g(xstar-eta))/(2*eta)\n",
    "k3, x3 = RelaxedAlgo(g,alpha,x0,eps,K)\n",
    "\n",
    "## vector e_{k} = |x_k-x^*|\n",
    "err1 = abs(x1-xstar)\n",
    "err2 = abs(x2-xstar)\n",
    "err3 = abs(x3-xstar)\n",
    "\n",
    "## vectors e_{k} and  e_{k+1}\n",
    "X1 = err1[:-1]    # X[k]=e_{k},   k=0...kend-1\n",
    "Y1 = err1[1:]     # Y[k]=e_{k+1}, k=0...kend-1\n",
    "X2 = err2[:-1]    # X[k]=e_{k},   k=0...kend-1\n",
    "Y2 = err2[1:]     # Y[k]=e_{k+1}, k=0...kend-1\n",
    "X3 = err3[:-1]    # X[k]=e_{k},   k=0...kend-1\n",
    "Y3 = err3[1:]     # Y[k]=e_{k+1}, k=0...kend-1\n",
    "\n",
    "## plot e_{k+1} versus e_{k} in loglog-scale\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "plt.loglog(X1, Y1, marker=\"o\", label='Inital Algorithm') #log-log scale\n",
    "plt.loglog(X2, Y2, marker=\"o\", label='Relaxed Algorithm')\n",
    "plt.loglog(X3, Y3, marker=\"o\", label='Approximated Relaxed Algorithm')\n",
    "plt.legend(loc='lower right', fontsize=18)\n",
    "plt.axis('equal')\n",
    "plt.xlabel('$e_k$', fontsize=18)\n",
    "plt.ylabel('$e_{k+1}$', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div  markdown=1 class=\"DoIt\">\n",
    "\n",
    "Comment on the results. In particular, how do the behavior of the initial and the approximated relaxed algorithm compare? Can you explain why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div  markdown=1 class=\"Answer\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div  markdown=1 class=\"Correction\">\n",
    "\n",
    "The relaxed algorithm yields a log-log curve with slope 2, whereas the two others have slope 1. The line for the relaxed algorithm breaks before the end because the machine precision has been reached for $e_{k+1}$.\n",
    "\n",
    "The initial and the approximated relaxed algorithms have the same order of convergence, equal to 1:\n",
    "$$e_{k+1}\\sim C e_k$$\n",
    "\n",
    "However, the constant $C$ is not the same for both algorithms. Indeed:\n",
    "- For the initial algorithm, the constant $C$ is given by (see the first theorem) $\\vert g'(x^*)\\vert \\approx 0.76$.\n",
    "- For the approximated relaxed algorithm, the constant $C$ is given by $\\vert g'_\\alpha(x^*)\\vert $, which should be close to $0$ and thus much smaller that $\\vert g'(x^*)\\vert $, since $\\alpha \\approx -g'(x^*)$.\n",
    "\n",
    "\n",
    "This difference in the multiplicative constant $C$ translates on a logarithmic scale into a lower intercept for the approximated relaxed algorithm.\n",
    "\n",
    "As a consequence, while both of these algorithms are of order 1, the approximated relaxed one still converges faster than the initial one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div  markdown=1 class=\"DoIt\">\n",
    "\n",
    "Look at the code and run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## New algorithm for function g\n",
    "## input : g = name of the function\n",
    "##         alpha = initial relaxation parameter\n",
    "##         x0 = initialization\n",
    "##         Niter = number of iterations between updates of alpha\n",
    "##         K = number of iterations\n",
    "## output : x = sequence generated using the fixed point iteration for g (x[k]=x_k)\n",
    "def NewAlgo(g,alpha,x0,eps,K,Niter):\n",
    "    x = np.zeros(K+1)\n",
    "    x[0] = x0 \n",
    "    print('k=0, alpha =',alpha)\n",
    "    x[1] = (g(x0) + alpha * x0) / (alpha +1)\n",
    "    k = 1\n",
    "    while abs(x[k]-x[k-1])>eps and k < K:\n",
    "        i = 0\n",
    "        while i < Niter and k < K:\n",
    "            print('k=',k,'alpha =',alpha)\n",
    "            x[k+1] = (g(x[k]) + alpha * x[k]) / (alpha +1)\n",
    "            k = k+1\n",
    "            i = i+1\n",
    "        alpha = -(g(x[k])-g(x[k-1]))/(x[k]-x[k-1])\n",
    "    return (k, x[:k+1])\n",
    "\n",
    "\n",
    "########################\n",
    "########################\n",
    "xstar = 2**(1/3)\n",
    "x0 = 2\n",
    "eps = 1e-8\n",
    "K = 30\n",
    "eta = 1\n",
    "alpha = -(g(xstar+eta)-g(xstar-eta))/(2*eta)\n",
    "Niter = 4\n",
    "k, x = NewAlgo(g,alpha,x0,eps,K,Niter)\n",
    "\n",
    "## vector e_{k} = |x_k-x^*|\n",
    "err = abs(x-xstar)\n",
    "\n",
    "## vectors e_{k} and  e_{k+1}\n",
    "X = err[:-1]    # X[k]=e_{k},   k=0...kend-1\n",
    "Y = err[1:]     # Y[k]=e_{k+1}, k=0...kend-1\n",
    "\n",
    "## plot e_{k+1} versus e_{k} in loglog-scale\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "plt.loglog(X, Y, marker=\"o\", label='Updated Relaxed Algorithm') #log-log scale\n",
    "plt.legend(loc='lower right', fontsize=18)\n",
    "plt.axis('equal')\n",
    "plt.xlabel('$e_k$', fontsize=18)\n",
    "plt.ylabel('$e_{k+1}$', fontsize=18)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div  markdown=1 class=\"DoIt\">\n",
    "\n",
    "Can you explain the resulting plot?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div  markdown=1 class=\"Answer\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div  markdown=1 class=\"Correction\">\n",
    "\n",
    "The algorithm above is a modified version of the approximated relaxation method, where the value of $\\alpha$ is regularly updated. Every $N_{iter}$ iterations, since $x_k$ is closer to $x^*$, $\\alpha$ gets closer to $-g'(x^*)$. Hence the constant $C$ in the convergence speed gets closer to zero.\n",
    "\n",
    "Here, $\\alpha$ is updated at step 5. As a consequence, one can observe 2 consecutive lines with slope 1 and the intercept decreasing inbetween."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# execute this part to modify the css style\n",
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"./style/custom3.css\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "600px",
    "left": "0px",
    "right": "1124px",
    "top": "107px",
    "width": "240.667px"
   },
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}